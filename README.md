# Mistral RAG Chatbot ğŸ§ ğŸ’¬

A Retrieval-Augmented Generation (RAG) based chatbot that answers questions from your own documents using **LangChain**, **FAISS**, and **Mistral** LLM via **Ollama**. Built with a simple and clean **Streamlit UI**.

---

## ğŸš€ Features

- ğŸ” Ask questions grounded in your document context
- ğŸ§  Local LLM support (via Ollama with Mistral)
- ğŸ—ƒï¸ Vector similarity search using FAISS or ChromaDB
- ğŸ’¡ LangChain integration for prompt chaining
- ğŸŒ Minimal, responsive Streamlit web interface

---

## ğŸ“¸ UI Preview

<img src="screenshots\dashboard.png" alt="Chatbot Dashboard" width="600"/>
<br><br>
<img src="screenshots\sample queries.png" alt="Sample Queries" width="600"/>

---

## ğŸ› ï¸ Setup Instructions

### 1. Clone the repository
```bash
git clone https://github.com/your-username/context-aware-chatbot.git
cd context-aware-chatbot
```

### 2. Create & Activate Virtual Environment

```bash
# Windows
python -m venv venv
venv\Scripts\activate

# macOS/Linux
python3 -m venv venv
source venv/bin/activate
```

### 3. Install Dependencies


```bash
pip install -r requirements.txt
```

### 4. Run Ollama and Pull the Model


| âš ï¸ Ensure Ollama is installed and running on your system.

```bash
ollama pull mistral
```

### 5. Launch the App


```bash
streamlit run app.py

```