{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "456be3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "# from langchain_community.llms import HuggingFacePipeline\n",
    "# from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "from langchain_community.llms import Ollama\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93107a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 1: Data Ingestion & Preprocessing ---\n",
    "def load_and_preprocess_blogs(blog_dir=\"blogs\"):\n",
    "    raw_texts = []\n",
    "    for filename in os.listdir(blog_dir):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            filepath = os.path.join(blog_dir, filename)\n",
    "            with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "                content = f.read()\n",
    "                # Assuming the first line is the title\n",
    "                lines = content.split('\\n', 1)\n",
    "                title = lines[0].replace(\"Title: \", \"\").strip() if lines else \"No Title\"\n",
    "                text_content = lines[1].strip() if len(lines) > 1 else content.strip()\n",
    "                raw_texts.append({\"title\": title, \"text\": text_content})\n",
    "    return raw_texts\n",
    "\n",
    "def chunk_texts(raw_texts, chunk_size=200, chunk_overlap=20):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "        add_start_index=True,\n",
    "    )\n",
    "    documents = []\n",
    "    for item in raw_texts:\n",
    "        chunks = text_splitter.create_documents([item[\"text\"]])\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            # Add title and chunk index to metadata\n",
    "            chunk.metadata = {\"title\": item[\"title\"], \"chunk_index\": i}\n",
    "            documents.append(chunk)\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8a115c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2 blog posts.\n",
      "Generated 31 document chunks.\n",
      "\n",
      "Sample Chunk:\n",
      "When people talk about high performance, they often focus on time management, discipline, or motivation. But the real engine behind consistent output is not time — it’s energy. Energy fuels our\n",
      "{'title': 'Energy Mastery: The Secret to Sustainable High Performance', 'chunk_index': 0}\n"
     ]
    }
   ],
   "source": [
    "# Load and chunk the data\n",
    "blog_data = load_and_preprocess_blogs()\n",
    "documents = chunk_texts(blog_data)\n",
    "\n",
    "print(f\"Loaded {len(blog_data)} blog posts.\")\n",
    "print(f\"Generated {len(documents)} document chunks.\")\n",
    "# Print a sample chunk to verify\n",
    "if documents:\n",
    "    print(\"\\nSample Chunk:\")\n",
    "    print(documents[0].page_content)\n",
    "    print(documents[0].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40ba209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading embeddings model: sentence-transformers/all-MiniLM-L6-v2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hobopk\\AppData\\Local\\Temp\\ipykernel_8884\\1831342769.py:7: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=model_name)\n",
      "d:\\My Work\\rag_test\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating FAISS vector store...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\My Work\\rag_test\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving FAISS index to faiss_index...\n",
      "FAISS index saved.\n"
     ]
    }
   ],
   "source": [
    "# --- Step 2: Embedding + Vector Store ---\n",
    "EMBEDDING_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "vector_store_path = \"faiss_index\"\n",
    "\n",
    "def create_and_save_vector_store(documents, model_name=EMBEDDING_MODEL_NAME, path=vector_store_path):\n",
    "    print(f\"\\nLoading embeddings model: {model_name}...\")\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=model_name)\n",
    "    print(\"Creating FAISS vector store...\")\n",
    "    vector_store = FAISS.from_documents(documents, embeddings)\n",
    "    print(f\"Saving FAISS index to {path}...\")\n",
    "    vector_store.save_local(path)\n",
    "    print(\"FAISS index saved.\")\n",
    "    return vector_store\n",
    "\n",
    "def load_vector_store(model_name=EMBEDDING_MODEL_NAME, path=vector_store_path):\n",
    "    print(f\"\\nLoading embeddings model: {model_name}...\")\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=model_name)\n",
    "    print(f\"Loading FAISS index from {path}...\")\n",
    "    vector_store = FAISS.load_local(path, embeddings, allow_dangerous_deserialization=True) # Added for safety, if loading an existing index\n",
    "    print(\"FAISS index loaded.\")\n",
    "    return vector_store\n",
    "\n",
    "# Create and save the vector store (run this once to generate the index)\n",
    "faiss_vector_store = create_and_save_vector_store(documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcd1d3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mistral via Ollama loaded.\n",
      "\n",
      "LangChain ConversationalRetrievalChain initialized.\n",
      "\n",
      "--- Testing RAG Pipeline ---\n",
      "\n",
      "Question 1: What is energy mastery?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hobopk\\AppData\\Local\\Temp\\ipykernel_8884\\27814328.py:48: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
      "  llm = Ollama(model=\"mistral\")\n",
      "d:\\My Work\\rag_test\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:  Energy mastery refers to a way of managing one's energy effectively, rather than simply focusing on working longer hours. It involves doing less, but better, by investing in recovery between periods of intense cognitive effort, making informed decisions that affect energy levels, and scheduling tasks during the time when energy is highest, often mornings.\n",
      "\n",
      "Question 2: How do high performers avoid burnout?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\My Work\\rag_test\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:  High performers use a method that involves structuring their weeks with cycles of push (working intensely) and pause (resting), recognizing rest as fuel, not weakness. They also focus on maintaining emotional hygiene through practices like journaling, coaching, mindfulness, and open communication with friends to prevent emotional bottlenecks.\n"
     ]
    }
   ],
   "source": [
    "# --- Step 3: LangChain Integration ---\n",
    "\"\"\"LLM_MODEL_NAME = \"gpt2\"\n",
    "\n",
    "# Initialize the LLM\n",
    "print(f\"\\nLoading LLM model: {LLM_MODEL_NAME}...\")\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(LLM_MODEL_NAME)\n",
    "    model = AutoModelForCausalLM.from_pretrained(LLM_MODEL_NAME)\n",
    "    # Ensure pad_token_id is set for GPT2 tokenizer\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token # Or any other suitable token\n",
    "    pipe = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        max_new_tokens=250, # Increased for potentially longer answers\n",
    "        temperature=0.7,\n",
    "        top_p=0.95,\n",
    "        repetition_penalty=1.15\n",
    "    )\n",
    "    llm = HuggingFacePipeline(pipeline=pipe)\n",
    "    print(\"LLM model loaded.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading LLM model {LLM_MODEL_NAME}: {e}\")\n",
    "    print(\"Attempting to load a smaller alternative (e.g., 'distilgpt2').\")\n",
    "    LLM_MODEL_NAME = \"distilgpt2\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(LLM_MODEL_NAME)\n",
    "    model = AutoModelForCausalLM.from_pretrained(LLM_MODEL_NAME)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    pipe = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        max_new_tokens=250,\n",
    "        temperature=0.7,\n",
    "        top_p=0.95,\n",
    "        repetition_penalty=1.15\n",
    "    )\n",
    "    llm = HuggingFacePipeline(pipeline=pipe)\n",
    "    print(f\"Successfully loaded {LLM_MODEL_NAME} as a fallback.\")\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "# Initialize Mistral via Ollama\n",
    "llm = Ollama(model=\"mistral\")\n",
    "print(\"Mistral via Ollama loaded.\")\n",
    "\n",
    "# Create a FAISS Retriever\n",
    "retriever = faiss_vector_store.as_retriever()\n",
    "\n",
    "# Define the prompt template\n",
    "prompt_template = \"\"\"You are a helpful assistant. Use the context below to answer the user's question.\n",
    "If the answer is not in the provided context, politely state that you don't have enough information.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "QA_CHAIN_PROMPT = PromptTemplate(input_variables=[\"context\", \"question\"], template=prompt_template)\n",
    "\n",
    "# Create the ConversationalRetrievalChain\n",
    "qa_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    combine_docs_chain_kwargs={\"prompt\": QA_CHAIN_PROMPT}\n",
    ")\n",
    "\n",
    "print(\"\\nLangChain ConversationalRetrievalChain initialized.\")\n",
    "\n",
    "# --- Sample Queries ---\n",
    "chat_history = []\n",
    "\n",
    "def get_answer_from_rag(query, history):\n",
    "    result = qa_chain.invoke({\"question\": query, \"chat_history\": history})\n",
    "    return result[\"answer\"]\n",
    "\n",
    "print(\"\\n--- Testing RAG Pipeline ---\")\n",
    "\n",
    "# Question 1: \"What is energy mastery?\"\n",
    "query1 = \"What is energy mastery?\"\n",
    "print(f\"\\nQuestion 1: {query1}\")\n",
    "response1 = get_answer_from_rag(query1, chat_history)\n",
    "print(f\"Answer: {response1}\")\n",
    "chat_history.append((query1, response1))\n",
    "\n",
    "# Question 2: \"How do high performers avoid burnout?\"\n",
    "query2 = \"How do high performers avoid burnout?\"\n",
    "print(f\"\\nQuestion 2: {query2}\")\n",
    "response2 = get_answer_from_rag(query2, chat_history)\n",
    "print(f\"Answer: {response2}\")\n",
    "chat_history.append((query2, response2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22559bf3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc594f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19f5ad5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.1)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
